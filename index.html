<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomouse Driving Agents</title>
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Anek+Devanagari&family=Nunito">
</head>
<body>
    <div>
        <h1>DOROTHIE: Spoken Dialogue for Handling Unexpected Situations in Interactive Autonomous Driving Agents</h1>
    </div>
    <div class="authors">
        <a href="" class="author">Ziqiao Ma,</a> <a href="" class="author">Ben VanDerPloeg,</a> <a href="" class="author">Cristian-Paul Bara,</a> 
        <a href="" class="author">Yidong Huang,</a> <a href="" class="author">Eui-In Kim,</a> <a href="" class="author">Felix Gervits,</a>
        <a href="" class="author">Matthew Marge,</a> <a href="" class="author">Joyce Chai</a>
    </div>
    <div class="conference">
        Conferece on Empirical Methods in Natural Language Processing (EMNLP 2022) 
        [<a href="https://arxiv.org/abs/2210.12511">pdf</a>][<a href="https://github.com/sled-group/DOROTHIE">code</a>]
    </div>
    <div class="section">
        <h2>Abstract</h2>
        <p>In the real world, autonomous driving agents navigate in highly dynamic environments full of unexpected situations 
            where pre-trained models are unreliable. In these situations, what is immediately available to vehicles is often 
            only human operators. Empowering autonomous driving agents with the ability to navigate in a continuous and dynamic 
            environment and to communicate with humans through sensorimotor-grounded dialogue becomes critical. To this end, we 
            introduce Dialogue On the ROad To Handle Irregular Events (DOROTHIE), a novel interactive simulation platform that 
            enables the creation of unexpected situations on the fly to support empirical studies on situated communication with 
            autonomous driving agents. Based on this platform, we created the Situated Dialogue Navigation (SDN), a navigation 
            benchmark of 183 trials with a total of 8415 utterances, around 18.7 hours of control streams, and 2.9 hours of trimmed 
            audio. SDN is developed to evaluate the agent's ability to predict dialogue moves from humans as well as generate its own 
            dialogue moves and physical navigation actions. We further developed a transformer-based baseline model for these 
            SDN tasks. Our empirical results indicate that language guided-navigation in a highly dynamic environment is an 
            extremely difficult task for end-to-end models. These results will provide insight towards future work on robust 
            autonomous driving agents.</p>
        <img src="images/DOROTHIE.jpg" alt="">
    </div>
    <div class="section">
        <h2>Dialogue On the ROad To Handle Irregular Events (DOROTHIE)</h2>

        <h3>Co-Wizard Interface</h3>
        <h3>Ad-Wizard Interface</h3>
    </div>
    <div class="section">
        <h2>Examples</h3>
    </div>
    <div class="section">
        <h2>Temporally-Ordered Task-Oriented (TOTO) Transformer</h2>
        <p>

        </p>
        <img src="images/TOTO.jpg" alt="">
    </div>
</body>
</html>